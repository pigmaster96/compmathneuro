\documentclass{report}
\title{`cs229'---Notes}
\date{Started 1st October 2024}
\author{Malcolm}
\usepackage{amsmath} %import math
\usepackage{mathtools} %more math
\usepackage{amssymb} %for QED symbol
\usepackage{amsthm} %
\usepackage{bm} %bolding without changing font
\usepackage{graphicx} %import imaging
\graphicspath{{./images/}} %set imaging path
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}} %matrix
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}} %matrix
\begin{document}
\maketitle

\chapter{Supervised learning}
Given a dataset of $n$ \textit{training examples} $\{(x^{(i)},y^{(i)});i=1\},\ldots,n\}$---a \textit{training set}---where $\bm{x}$ 
represents the \textit{features} and $\bm{y}$ the ``output'' or \textit{target} variable we are trying to predict.
If not already obvious, we denote the vector space of $\bm{x}$ as $\mathcal{X}$ and that of the outputs $\bm{y}$ as 
$\mathcal{Y}$.\\
\vspace{1mm}\\
Our goal is, given a training set, to learn a function $h:\mathcal{X}\mapsto\mathcal{Y}$ so that $h(x)$ is a 
``good'' predictor for the corresponding $y$. This function $h$ is called a \textit{hypothesis}.\\
\vspace{1mm}\\
When trying to predict a continuous target variable, we call this a \textit{regression} problem; whereas when $y$
can take on only a small number of discrete values we call that a \textit{classification} problem.

\subsection{Linear Regression}
Say we decide to approximate $y$ as a linear function of $x$:
\begin{equation*}
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\ldots
\end{equation*}
Where $\bm{\theta}$ represents the \textit{parameters/weights} (parametrising the
space of linear functions mapping from $\mathcal{X}$ to $\mathcal{Y}$).
We can simplify our notation as such: (by convention letting $x_0=1$, aptly named the \textit{intercept} term)
\begin{equation*}
h(x)=\sum^d_{i=0}\theta_ix_i=\bm{\theta}^T\bm{x}
\end{equation*}
In order to formalise a measure of proximity between the predicted value $h(x)$ and the target $y$,
we define a \textit{cost function}:
\begin{equation*}
J(\theta)=\frac{1}{2}\sum^n_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2
\end{equation*}
This particular cost function implies an \textit{ordinary least squares} regression model.
\newpage

\subsection{LMS algorithm}
Our cost function $J(\theta)$ gives us a measure of prediction accuracy. We want to choose $\theta$ so as to
minimise $J(\theta)$. Starting with an initial set of $\theta$, we need a search algorithm that
repeatedly changes $\theta$ in an attempt to minimise $J(\theta)$. Here we consider the \textit{gradient descent}
algorithm, which, given some initial $\theta$, repeatedly performs the update:
\begin{equation*}
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)
\end{equation*}
Where the update is simultaneously performed for all values of $j=0,\ldots,d$. $\alpha$ is called the 
\textit{learning rate}
(how much we move in the direction the gradient points in).\\
\vspace{1mm}\\
\textbf{Intuition}\\
Consider attempting to minimise the least mean squares (LMS) cost function for a single training example:
\begin{align*}
\frac{\partial}{\partial\theta_j}J(\theta)&=\frac{\partial}{\partial\theta_j}\frac{1}{2}(h_\theta(x)-y)^2\\
&=2\cdot\frac{1}{2}(h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}(h_\theta(x)-y)\\
&=(h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}\left(\sum^d_{i=0}\theta_ix_i-y\right)\\
&=(h_\theta(x)-y)\,x_j
\end{align*}
This gives us the update rule:
\begin{equation*}
\theta_j:=\theta_j+\alpha\left(y^{(i)}-h_\theta(x^{(i)})\right)x_j^{(i)}
\end{equation*}
(we use the notation $a:=b$ to denote (in a script) overwriting $a$ with $b$) Notice the property of the LMS update
rule that the magnitude of the update is proportional to the \textit{error} term 
$\left(y^{(i)}-h_\theta(x^{(i)})\right)$; this means that predictions further off the mark result in a greater
correction to $\theta$.\\
(next page)
\newpage
\noindent\textbf{Batch Gradient Descent}\\
We had the LMS rule for when there was only a single training example. One way to modify this method for a
training set of more than one example is the following algorithm:\\
\vspace{1mm}\\
\indent Repeat until convergence \{
\begin{equation*}
\theta_j:=\theta_j+\alpha\sum^n_{i=1}\left(y^{(i)}-h_\theta(x^{(i)})\right)x_j^{(i)},\text{ (for every $j$)}
\end{equation*}
\indent\}\\
\vspace{1mm}\\
Written more succinctly ($\theta_j$ and $x_j$ as vectors):
\begin{equation*}
\theta:=\theta+\alpha\sum^n_{i=1}\left(y^{(i)}-h_\theta(x^{(i)})\right)x^{(i)}
\end{equation*}
This method looks at every example in the entire training set on every step, and is called 
\textit{batch gradient descent}.\\
\vspace{1mm}\\
\textbf{Stochastic gradient descent}\\
Now consider another algorithm:\\
\vspace{1mm}\\
\indent Loop \{\\
\indent\indent for $i=1$ to $n$, \{
\begin{equation*}
\theta_j:=\theta_j+\alpha\left(y^{(i)}-h_\theta(x^{(i)})\right)x_j^{(i)},\quad\text{(for every $j$)}
\end{equation*}
\indent\indent\}\\
\indent\}\\
\vspace{1mm}\\
Written more compactly:
\begin{equation*}
\theta:=\theta+\alpha\left(y^{(i)}-h_\theta(x^{(i)})\right)x^{(i)},\text{ (update $\theta$ $n$ times)}
\end{equation*}
Here we update $\theta$ for each training example during each run of the training set. This is called
\textit{stochastic/incremental gradient descent}.\\
\vspace{1mm}\\
Whereas batch gradient descent has to scan through the entire training set before taking a single step---which 
is costly if $n$ is large---stochastic gradient descent continues to make progress with each example it looks at.
Stochastic gradient descent gets $\theta$ ``close'' to the minimum much faster than batch gradient descent. Note
that ``convergence'' doesn't really occur---the parameters $\theta$ will keep oscillating around the minimum of
$J(\theta)$. (though most values near minimum would be reasonably good approximations to the true minimum)
\newpage

\subsection{Gradient/Hessian of $b^Tx$, $x^TAx$}
\textbf{Matrix Derivatives}\\
For a function $f:\mathbb{R}^{n\times d}\mapsto\mathbb{R}$ mapping from $n$-by-$d$ matrices to real numbers,
we define the derivative of $f$ with respect to $A$ to be
\begin{equation*}
\nabla_Af(A)=\begin{bmatrix}
\frac{\partial f}{\partial A_{11}}&\cdots&\frac{\partial f}{\partial A_{1d}}\\
\vdots&\ddots&\vdots\\
\frac{\partial f}{\partial A_{n1}}&\cdots&\frac{\partial f}{\partial A_{nd}}
\end{bmatrix}
\end{equation*}
\textbf{Gradient of} $b^Tx$:\\
For $x\in\mathbb{R}^n$ and $f(x)=b^Tx$ for known $b\in\mathbb{R}^n$, we have
\begin{equation*}
f(x)=\sum^n_{i=1}b_ix_i
\end{equation*}
and so
\begin{equation*}
\frac{\partial f(x)}{\partial x_k}=\frac{\partial}{\partial x_k}\sum^n_{i=1}b_ix_i=b_k
\end{equation*}
Consider repeating this for the partial of each element of $x$. See that $\nabla_xb^Tx=b$ (analagous to
single variable calculus).\\
\vspace{1mm}\\
\textbf{Gradient of} $f(x)=x^TAx$:\\
Now consider the quadratic function $f(x)=x^TAx$ for $A\in\mathbb{S}^n$ (meaning symmetric). First see that
\begin{equation*}
f(x)=\sum^n_{i=1}\sum^n_{j=1}A_{ij}x_ix_j
\end{equation*}
this can be seen from considering that 
\begin{equation*}
b=Ax\in\mathbb{R}^{n\times1}
\end{equation*}
can be written as
\begin{align*}
b_1=&\sum^n_{i=1}A_{1i}x_i\\
&\vdots\\
b_n=&\sum^n_{i=1}A_{ni}x_i
\end{align*}
(next page)
\newpage
\noindent so we can write
\begin{align*}
x^TAx=x^T&b=\sum^n_{j=1}x_jb_j\in\mathbb{R}\\
=&\sum^n_{j=1}x_j\left(\sum^n_{i=1}A_{ji}x_i\right)
\end{align*}
Rewriting gives us
\begin{equation*}
f(x)=\sum^n_{i=1}\sum^n_{j=1}A_{ij}x_ix_j
\end{equation*}
which was what we wanted. Now we take the partial derivative by considering the terms including $x_k$ an
$x_k^2$ factors separately:
\begin{align*}
\frac{\partial f(x)}{\partial x_k}&=\frac{\partial f(x)}{\partial x_k}\sum^n_{i=1}\sum^n_{j=1}A_{ij}x_ix_j\\
&=\frac{\partial f(x)}{\partial x_k}\left[\sum_{i\neq k}
\sum_{j\neq k}A_{ij}x_ix_j+\sum_{i\neq k}A_{ik}x_ix_k
+\sum_{j\neq k}A_{kj}x_kx_j+A_{kk}x_k^2\right]\\
&=\sum_{i\neq k}A_{ik}x_i+\sum_{j\neq k}A_{kj}x_j+2A_{kk}x_k\\
&=\sum_{i=1}A_{ik}x_i+\sum_{j=1}A_{kj}x_j=2\sum^n_{i=1}A_{ki}x_i
\end{align*}
where the last equality follows since $A$ is assumed to be \textit{symmetric}. Since 
\begin{equation*}
\sum^n_{i=1}A_{ki}x_i
\end{equation*}
is just the inner product of a single row of $A$ and $x$---the $k$th entry of $\nabla_xf(x)$ is the just the 
inner product of the $k$th row of $A$ and $x$; therefore 
\begin{equation*}
\nabla_xx^TAx=2Ax
\end{equation*}
also analagous to single variable calculus.\\
(next page)
\newpage
\noindent\textbf{Hessian}\\
Finally we consider the Hessian of the quadratic function $f(x)=x^TAx$ (it should be obvious
that the Hessian of a linear function $b^Tx$ is zero):
\begin{equation*}
\frac{\partial^2f(x)}{\partial x_k\partial x_\ell}=
\frac{\partial}{\partial x_k}\left[\frac{\partial f(x)}{\partial x_\ell}\right]
=\frac{\partial}{\partial x_k}\left[2\sum^n_{i=1}A_{\ell i}x_i\right]=2A_{\ell k}=2A_{k\ell}
\end{equation*}
See therefore that $\nabla^2_xx^TAx=2A$.\\
\vspace{1mm}\\
\textbf{Recapitulation}:
\begin{itemize}
\item$\nabla_xb^Tx=b$
\item$\nabla_xx^TAx=2Ax$
\item$\nabla^2_xx^TAx=2A$
\end{itemize}
\newpage

\subsection{Least Squares matrix representation}
Here we consider $J(\theta)$---the least squares cost function---in matrix-vectorial notation.\\
\vspace{1mm}\\
Given a training set, we define the \textit{design matrix} $X$ to be the $n\times d$ matrix ($n\times d+1$ if we
include the intercept term) that contains the training examples' inputs values in its rows:
\begin{equation*}
X=\begin{bmatrix}
\horzbar(x^{(1)})^T\horzbar\\
\horzbar(x^{(2)})^T\horzbar\\
\vdots\\\horzbar(x^{(n)})^T\horzbar
\end{bmatrix}
\end{equation*}
and $y$ the $n$-dimensional vector containing the target values:
\begin{equation*}
y=\begin{bmatrix}
y^{(1)}\\y^{(2)}\\\vdots\\y^{(n)}
\end{bmatrix}
\end{equation*}
Now we try to represent the least squares function over the entire dataset; see that since 
\begin{equation*}
h_\theta(x^{(i)})=(x^{(i)})^T\theta
\end{equation*}
we can write
\begin{align*}
X\theta-y&=\begin{bmatrix}
(x^{(1)})^T\theta\\
\vdots\\(x^{(n)})^T\theta
\end{bmatrix}-
\begin{bmatrix}y^{(1)}\\\vdots\\y^{(n)}\end{bmatrix}\\
&=\begin{bmatrix}
h_\theta(x^{(1)})-y^{(1)}\\\vdots\\
h_\theta(x^{(n)})-y^{(n)}\end{bmatrix}\in\mathbb{R}^n
\end{align*}
Using the fact that for a vector $z$, 
$z^Tz=\sum_iz^2_i$:
\begin{align*}
\frac{1}{2}(X\theta-y)^T(X\theta-y)&=
\frac{1}{2}\sum^n_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2\\
&=J(\theta)
\end{align*}
(next page)\newpage
\noindent\textbf{Minimising the LMS}\\
To minimise the cost function $J(\theta)$ we want its derivatives with respect to $\theta$:
\begin{align*}
\nabla_\theta J(\theta)&=\nabla_\theta\frac{1}{2}(X\theta-y)^T(X\theta-y)\\
&=\frac{1}{2}\nabla_\theta\left((X\theta)^T-y^T\right)(X\theta-y)\\
&=\frac{1}{2}\nabla_\theta(\theta^TX^TX\theta
-y^TX\theta-(X\theta)^Ty-y^Ty)\\
&=\frac{1}{2}\nabla_\theta(\theta^TX^TX\theta
-y^TX\theta-y^TX\theta)\\
&=\frac{1}{2}\nabla_\theta(\theta^TX^TX\theta-2(X^Ty)\theta)\\
&=\frac{1}{2}(2X^TX\theta-2X^Ty)\\
&=X^TX\theta-X^Ty
\end{align*}
Of note:
\begin{itemize}
\item The third equality comes from $(Ab)^T=b^TA^T$
\item The fourth equality comes from $a^Tb=b^Ta$
\item To differentiate we use the facts $\nabla_xb^Tx=b$ and $\nabla_xx^TAx=2Ax$. 
Note the second fact assumes $A$ is symmetric, which is true here since $X^TX$ is symmetric.
\end{itemize}
To minimise $J$ we set the derivative to zero and obtain the \textit{normal equations}:
\begin{equation*}
X^TX\theta=X^Ty
\end{equation*}
Thus we have, in closed-form, the value of $\theta$ that minimises $J(\theta)$, given by
\begin{equation*}
\theta=(X^TX)^{-1}X^Ty
\end{equation*}
Note that this final step implicitly assumes that $X^TX$ is invertible. This can be checked before calculating
the inverse; if either the number of linearly independent examples is fewer than the number of
features, or if the features are not linearly independent:\\\vspace{1mm}\\
See that a matrix $A\in\mathbb{R}^{n\times m}$ can have rank $m$ ($=\min(n,m)$) only if $n\geq m$ 
(thus more linearly independent examples than features are required---$m$ representing features and $n$ samples.) 
If the features ($m$) were not linearly independent, the matrix $A$ would not have rank $m$. See that
\begin{align*}
A^TAx=0&\implies x^TA^TAx=0\implies(Ax)^TAx=0\\&\implies||Ax||^2=0\implies Ax=0
\end{align*}
Intuitively, only if $Ax=0$ is the trivial solution (meaning $A$ is a one-to-one mapping for $m$ sized vectors), 
then would $A^TAx$ also be one-to-one and also invertible.
\newpage














\end{document}
