\documentclass{report}
\title{`cs229'---Notes}
\date{Started 1st October 2024}
\author{Malcolm}
\usepackage{amsmath} %import math
\usepackage{mathtools} %more math
\usepackage{amssymb} %for QED symbol
\usepackage{amsthm} %
\usepackage{bm} %bolding without changing font
\usepackage{graphicx} %import imaging
\graphicspath{{./images/}} %set imaging path
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}} %matrix
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}} %matrix
\begin{document}
\maketitle

\chapter{Supervised learning}
Given a dataset of $n$ \textit{training examples} $\{(x^{(i)},y^{(i)});i=1\},\ldots,n\}$---a \textit{training set}---where $\bm{x}$ 
represents the \textit{features} and $\bm{y}$ the ``output'' or \textit{target} variable we are trying to predict.
If not already obvious, we denote the vector space of $\bm{x}$ as $\mathcal{X}$ and that of the outputs $\bm{y}$ as 
$\mathcal{Y}$.\\
\vspace{1mm}\\
Our goal is, given a training set, to learn a function $h:\mathcal{X}\mapsto\mathcal{Y}$ so that $h(x)$ is a 
``good'' predictor for the corresponding $y$. This function $h$ is called a \textit{hypothesis}.\\
\vspace{1mm}\\
When trying to predict a continuous target variable, we call this a \textit{regression} problem; whereas when $y$
can take on only a small number of discrete values we call that a \textit{classification} problem.
\subsection{Linear Regression}
Say we decide to approximate $y$ as a linear function of $x$:
\begin{equation*}
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\ldots
\end{equation*}
Where $\bm{\theta}$ represents the \textit{parameters/weights} (parametrising the
space of linear functions mapping from $\mathcal{X}$ to $\mathcal{Y}$).
We can simplify our notation as such: (by convention letting $x_0=1$, aptly named the \textit{intercept} term)
\begin{equation*}
h(x)=\sum^d_{i=0}\theta_ix_i=\bm{\theta}^T\bm{x}
\end{equation*}
In order to formalise a measure of proximity between the predicted value $h(x)$ and the target $y$,
we define a \textit{cost function}:
\begin{equation*}
J(\theta)=\frac{1}{2}\sum^n_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2
\end{equation*}
This particular cost function implies an \textit{ordinary least squares} regression model.
\newpage
\subsection{LMS algorithm}
Our cost function $J(\theta)$ gives us a measure of prediction accuracy. We want to choose $\theta$ so as to
minimise $J(\theta)$. Starting with an initial set of $\theta$, we need a search algorithm that
repeatedly changes $\theta$ in an attempt to minimise $J(\theta)$. Here we consider the \textit{gradient descent}
algorithm, which, given some initial $\theta$, repeatedly performs the update:
\begin{equation*}
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)
\end{equation*}
Where the update is simultaneously performed for all values of $j=0,\ldots,d$. $\alpha$ is called the 
\textit{learning rate}
(how much we move in the direction the gradient points in).\\
\vspace{1mm}\\
\textbf{Intuition}\\
Consider attempting to minimise the least mean squares (LMS) cost function for a single training example:
\begin{align*}
\frac{\partial}{\partial\theta_j}J(\theta)&=\frac{\partial}{\partial\theta_j}\frac{1}{2}(h_\theta(x)-y)^2\\
&=2\cdot\frac{1}{2}(h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}(h_\theta(x)-y)\\
&=(h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}\left(\sum^d_{i=0}\theta_ix_i-y\right)\\
&=(h_\theta(x)-y)\,x_j
\end{align*}
This gives us the update rule:
\begin{equation*}
\theta_j:=\theta_j+\alpha\left(y^{(i)}-h_\theta(x^{(i)})\right)x_j^{(i)}
\end{equation*}
(we use the notation $a:=b$ to denote (in a script) overwriting $a$ with $b$) Notice the property of the LMS update
rule that the magnitude of the update is proportional to the \textit{error} term 
$\left(y^{(i)}-h_\theta(x^{(i)})\right)$; this means that predictions further off the mark result in a greater
correction to $\theta$.\\
(next page)
\newpage
\noindent\textbf{Batch Gradient Descent}\\
We had the LMS rule for when there was only a single training example. One way to modify this method for a
training set of more than one example is the following algorithm:\\
\vspace{1mm}\\
\indent Repeat until convergence \{
\begin{equation*}
\theta_j:=\theta_j+\alpha\sum^n_{i=1}\left(y^{(i)}-h_\theta(x^{(i)})\right)x_j^{(i)},\text{ (for every $j$)}
\end{equation*}
\indent\}\\
\vspace{1mm}\\
Written more succinctly ($\theta_j$ and $x_j$ as vectors):
\begin{equation*}
\theta:=\theta+\alpha\sum^n_{i=1}\left(y^{(i)}-h_\theta(x^{(i)})\right)x^{(i)}
\end{equation*}
This method looks at every example in the entire training set on every step, and is called 
\textit{batch gradient descent}.\\
\vspace{1mm}\\
\textbf{Stochastic gradient descent}\\
Now consider another algorithm:\\
\vspace{1mm}\\
\indent Loop \{\\
\indent\indent for $i=1$ to $n$, \{
\begin{equation*}
\theta_j:=\theta_j+\alpha\left(y^{(i)}-h_\theta(x^{(i)})\right)x_j^{(i)},\quad\text{(for every $j$)}
\end{equation*}
\indent\indent\}\\
\indent\}\\
\vspace{1mm}\\
Written more compactly:
\begin{equation*}
\theta:=\theta+\alpha\left(y^{(i)}-h_\theta(x^{(i)})\right)x^{(i)},\text{ (update $\theta$ $n$ times)}
\end{equation*}
Here we update $\theta$ for each training example during each run of the training set. This is called
\textit{stochastic/incremental gradient descent}.\\
\vspace{1mm}\\
Whereas batch gradient descent has to scan through the entire training set before taking a single step---which 
is costly if $n$ is large---stochastic gradient descent continues to make progress with each example it looks at.
Stochastic gradient descent gets $\theta$ ``close'' to the minimum much faster than batch gradient descent. Note
that ``convergence'' doesn't really occur---the parameters $\theta$ will keep oscillating around the minimum of
$J(\theta)$. (though most values near minimum would be reasonably good approximations to the true minimum)
\newpage
\subsection{Gradients of }
\textbf{Matrix Derivatives}\\
For a function $f:\mathbb{R}^{n\times d}\mapsto\mathbb{R}$ mapping from $n$-by-$d$ matrices to real numbers,
we define the derivative of $f$ with respect to $A$ to be
\begin{equation*}
\nabla_Af(A)=\begin{bmatrix}
\frac{\partial f}{\partial A_{11}}&\cdots&\frac{\partial f}{\partial A_{1d}}\\
\vdots&\ddots&\vdots\\
\frac{\partial f}{\partial A_{n1}}&\cdots&\frac{\partial f}{\partial A_{nd}}
\end{bmatrix}
\end{equation*}
\textbf{Gradient of} $b^Tx$:\\
For $x\in\mathbb{R}^n$ and $f(x)=b^Tx$ for known $b\in\mathbb{R}^n$, we have
\begin{equation*}
f(x)=\sum^n_{i=1}b_ix_i
\end{equation*}
and so
\begin{equation*}
\frac{\partial f(x)}{\partial x_k}=\frac{\partial}{\partial x_k}\sum^n_{i=1}b_ix_i=b_k
\end{equation*}
Consider repeating this for the partial of each element of $x$. See that $\nabla_xb^Tx=b$ (analagous to
single variable calculus).\\
\vspace{1mm}\\
\textbf{Gradient of} $f(x)=x^TAx$:\\
Now consider the quadratic function $f(x)=x^TAx$ for $A\in\mathbb{S}^n$ (meaning symmetric). First see that
\begin{equation*}
f(x)=\sum^n_{i=1}\sum^n_{j=1}A_{ij}x_ix_j
\end{equation*}
this can be seen from considering that 
\begin{equation*}
b=Ax\in\mathbb{R}^{n\times1}
\end{equation*}
can be written as
\begin{align*}
b_1=&\sum^n_{i=1}A_{1i}x_i\\
&\vdots\\
b_n=&\sum^n_{i=1}A_{ni}x_i
\end{align*}
(next page)
\newpage
\noindent so we can write
\begin{align*}
x^TAx=x^T&b=\sum^n_{j=1}x_jb_j\in\mathbb{R}\\
=&\sum^n_{j=1}x_j\left(\sum^n_{i=1}A_{ji}x_i\right)
\end{align*}
Rewriting gives us
\begin{equation*}
f(x)=\sum^n_{i=1}\sum^n_{j=1}A_{ij}x_ix_j
\end{equation*}
which was what we wanted. Now we take the partial derivative by considering the terms including $x_k$ an
$x_k^2$ factors separately:
\begin{align*}
\frac{\partial f(x)}{\partial x_k}&=\frac{\partial f(x)}{\partial x_k}\sum^n_{i=1}\sum^n_{j=1}A_{ij}x_ix_j\\
&=\frac{\partial f(x)}{\partial x_k}\left[\sum_{i\neq k}
\sum_{j\neq k}A_{ij}x_ix_j+\sum_{i\neq k}A_{ik}x_ix_k
+\sum_{j\neq k}A_{kj}x_kx_j+A_{kk}x_k^2\right]\\
&=\sum_{i\neq k}A_{ik}x_i+\sum_{j\neq k}A_{kj}x_j+2A_{kk}x_k\\
&=\sum_{i=1}A_{ik}x_i+\sum_{j=1}A_{kj}x_j=2\sum^n_{i=1}A_{ki}x_i
\end{align*}
where the last equality follows since $A$ is assumed to be \textit{symmetric}. Since 
\begin{equation*}
\sum^n_{i=1}A_{ki}x_i
\end{equation*}
is just the inner product of a single row of $A$ and $x$---the $k$th entry of $\nabla_xf(x)$ is the just the 
inner product of the $k$th row of $A$ and $x$; therefore 
\begin{equation*}
\nabla_xx^TAx=2Ax
\end{equation*}
also analagous to single variable calculus.
(next page)
\newpage
\noindent\textbf{Hessian}

\newpage

\subsection{Least Squares matrix representation}
Here we consider $J(\theta)$---the least squares cost function---in matrix-vectorial notation.\\
\vspace{1mm}\\
Given a training set, we define the \textit{design matrix} $\bm{X}$ to be the $n\times d$ matrix ($n\times d+1$ if we
include the intercept term) that contains the training examples' inputs values in its rows:
\begin{equation*}
\bm{X}=\begin{bmatrix}
\horzbar(x^{(1)})^T\horzbar\\
\horzbar(x^{(2)})^T\horzbar\\
\vdots\\\horzbar(x^{(n)})^T\horzbar
\end{bmatrix}
\end{equation*}
and $\bm{y}$ the $n$-dimensional vector containing the target values:
\begin{equation*}
\bm{y}=\begin{bmatrix}
y^{(1)}\\y^{(2)}\\\vdots\\y^{(n)}
\end{bmatrix}
\end{equation*}
See that we can represent our least squares function over the entire dataset as









\end{document}
