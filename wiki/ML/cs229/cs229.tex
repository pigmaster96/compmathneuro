\documentclass{report}
\title{`cs229'---Notes}
\date{Started 1st October 2024}
\author{Malcolm}
\usepackage{amsmath} %import math
\usepackage{mathtools} %more math
\usepackage{amssymb} %for QED symbol
\usepackage{amsthm} %
\usepackage{bm} %bolding without changing font
\usepackage{graphicx} %import imaging
\graphicspath{{./images/}} %set imaging path
\begin{document}
\maketitle

\chapter{Supervised learning}
Given a dataset of $n$ \textit{training examples} $\{(x^{(i)},y^{(i)});i=1\},\ldots,n\}$---a \textit{training set}---where $\bm{x}$ 
represents the \textit{features} and $\bm{y}$ the ``output'' or \textit{target} variable we are trying to predict.
If not already obvious, we denote the vector space of $\bm{x}$ as $\mathcal{X}$ and that of the outputs $\bm{y}$ as 
$\mathcal{Y}$.\\
\vspace{1mm}\\
Our goal is, given a training set, to learn a function $h:\mathcal{X}\mapsto\mathcal{Y}$ so that $h(x)$ is a 
``good'' predictor for the corresponding $y$. This function $h$ is called a \textit{hypothesis}.\\
\vspace{1mm}\\
When trying to predict a continuous target variable, we call this a \textit{regression} problem; whereas when $y$
can take on only a small number of discrete values we call that a \textit{classification} problem.
\subsection{Linear Regression}
Say we decide to approximate $y$ as a linear function of $x$:
\begin{equation*}
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\ldots
\end{equation*}
Where $\bm{\theta}$ represents the \textit{parameters/weights} (parametrising the
space of linear functions mapping from $\mathcal{X}$ to $\mathcal{Y}$).
We can simplify our notation as such: (by convention letting $x_0=1$, aptly named the \textit{intercept} term)
\begin{equation*}
h(x)=\sum^d_{i=0}\theta_ix_i=\bm{\theta}^T\bm{x}
\end{equation*}
In order to formalise a measure of proximity between the predicted value $h(x)$ and the target $y$,
we define a \textit{cost function}:
\begin{equation*}
J(\theta)=\frac{1}{2}\sum^n_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2
\end{equation*}
This particular cost function implies an \textit{ordinary least squares} regression model.
\newpage
\subsection{LMS algorithm}
Our cost function $J(\theta)$ gives us a measure of prediction accuracy. We want to choose $\theta$ so as to
minimise $J(\theta)$. Starting with an initial set of $\theta$, we need a search algorithm that
repeatedly changes $\theta$ in an attempt to minimise $J(\theta)$. Here we consider the \textit{gradient descent}
algorithm, which, given some initial $\theta$, repeatedly performs the update:
\begin{equation*}
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)
\end{equation*}
Where the update is simultaneously performed for all values of $j=0,\ldots,d$. $\alpha$ is called the 
\textit{learning rate}
(how much we move in the direction the gradient points in).\\
\vspace{1mm}\\
\textbf{Intuition}\\
Consider attempting to minimise the least mean squares (LMS) cost function for a single training example:
\begin{align*}
\frac{\partial}{\partial\theta_j}J(\theta)&=\frac{\partial}{\partial\theta_j}\frac{1}{2}(h_\theta(x)-y)^2\\
&=2\cdot\frac{1}{2}(h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}(h_\theta(x)-y)\\
&=(h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}\left(\sum^d_{i=0}\theta_ix_i-y\right)\\
&=(h_\theta(x)-y)\,x_j
\end{align*}
This gives us the update rule:
\begin{equation*}
\theta_j:=\theta_j+\alpha\left(y^{(i)}-h_\theta(x^{(i)})\right)x_j^{(i)}
\end{equation*}
(we use the notation $a:=b$ to denote (in a script) overwriting $a$ with $b$) Notice the property of the LMS update
rule that the magnitude of the update is proportional to the \textit{error} term 
$\left(y^{(i)}-h_\theta(x^{(i)})\right)$; this means that predictions further off the mark result in a greater
correction to $\theta$.\\
(next page)
\newpage
\noindent\textbf{Batch Gradient Descent}\\
We had the LMS rule for when there was only a single training example. One way to modify this method for a
training set of more than one example is the following algorithm:\\
\vspace{1mm}\\
\indent Repeat until convergence \{
\begin{equation*}
\theta_j:=\theta_j+\alpha\sum^n_{i=1}\left(y^{(i)}-h_\theta(x^{(i)})\right)x_j^{(i)},\text{ (for every $j$)}
\end{equation*}
\indent\}\\
\vspace{1mm}\\
Written more succinctly ($\theta_j$ and $x_j$ as vectors):
\begin{equation*}
\theta:=\theta+\alpha\sum^n_{i=1}\left(y^{(i)}-h_\theta(x^{(i)})\right)x^{(i)}
\end{equation*}
This method looks at every example in the entire training set on every step, and is called 
\textit{batch gradient descent}.\\
\vspace{1mm}\\
\textbf{Stochastic gradient descent}\\
Now consider another algorithm:\\
\vspace{1mm}\\
\indent Loop \{\\
\indent\indent for $i=1$ to $n$, \{
\begin{equation*}
\theta_j:=\theta_j+\alpha\left(y^{(i)}-h_\theta(x^{(i)})\right)x_j^{(i)},\quad\text{(for every $j$)}
\end{equation*}
\indent\indent\}\\
\indent\}\\
\vspace{1mm}\\
Written more compactly:
\begin{equation*}
\theta:=\theta+\alpha\left(y^{(i)}-h_\theta(x^{(i)})\right)x^{(i)},\text{ (update $\theta$ $n$ times)}
\end{equation*}
Here we update $\theta$ for each training example during each run of the training set. This is called
\textit{stochastic/incremental gradient descent}.\\
\vspace{1mm}\\
Whereas batch gradient descent has to scan through the entire training set before taking a single step---which 
is costly if $n$ is large---stochastic gradient descent continues to make progress with each example it looks at.
Stochastic gradient descent gets $\theta$ ``close'' to the minimum much faster than batch gradient descent. Note
that ``convergence'' doesn't really occur---the parameters $\theta$ will keep oscillating around the minimum of
$J(\theta)$. (though most values near minimum would be reasonably good approximations to the true minimum)
\newpage
\subsection{}






\end{document}
